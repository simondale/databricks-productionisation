trigger: 
- master

stages:

- stage: Build
  displayName: Build artefacts
  variables:
    - group: dev
  pool: 
    vm-image: ubuntu-latest
  jobs:
  - job:
    displayName: Create artefact
    steps:
      - bash: |
          echo {} > ~/.databricks-connect
          conda env create --force --name databricks --file conda.yml
        displayName: Setup environment
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python setup.py develop
        displayName: Build library
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          pytest --disable-warnings -rp -vv --cov-report xml:cov.xml --cov=. -o junit_family=xunit2 --junitxml=TEST-iris_model.xml tests/
        displayName: Execute unit tests
        env:
            SPARK_LOCAL_IP: 127.0.0.1
            DATABRICKS_ADDRESS: $(DatabricksUrl)
            DATABRICKS_API_TOKEN: $(DatabricksToken)
            DATABRICKS_ORG: $(DatabricksOrg)
            DATABRICKS_CLUSTER_ID: $(DatabricksCluster)
            DATABRICKS_PORT: 15001
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python setup.py bdist_wheel
        displayName: Build library
      - task: CopyFiles@2
        displayName: Copy Assets to Staging Folder
        inputs:
          Contents: |
            scripts/**py
            driver/**.py
            **/TEST-*.xml
            dist/**.whl
          TargetFolder: $(Build.ArtifactStagingDirectory)
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          databricks workspace mkdirs /Experiments
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/dist dbfs:/models/
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/driver dbfs:/driver/
        displayName: Upload to Databricks
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python scripts/submit.py --file dbfs:/driver/datapipeline.py
        displayName: Execute data pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python scripts/submit.py --file dbfs:/driver/features.py
        displayName: Execute features pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python scripts/submit.py --file dbfs:/driver/training.py
        displayName: Execute training pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(conda shell.bash hook)"
          conda activate databricks
          python scripts/submit.py --file dbfs:/driver/serving.py
        displayName: Execute serving pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - task: PublishBuildArtifacts@1
        displayName: Publish Artefacts
        inputs:
          PathtoPublish: $(Build.ArtifactStagingDirectory)
          ArtifactName: drop
          publishLocation: Container
      - task: PublishTestResults@2
        inputs:
            testResultsFormat: JUnit
            testResultsFiles: '**/TEST-*.xml'
            searchFolder: $(Build.ArtifactStagingDirectory)/drop/
            mergeTestResults: true
            failTaskOnFailedTests: true
            testRunTitle: Pipeline Tests


