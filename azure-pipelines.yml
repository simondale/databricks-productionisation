trigger: 
- master

stages:

- stage: Build
  displayName: Build artefacts
  variables:
    - group: dev
  pool: 
    vm-image: ubuntu-latest
  jobs:
  - job:
    displayName: Create artefact
    steps:
      - bash: |
          echo {} > ~/.databricks-connect
          conda env create --force --name $ENV --file conda.yml
          conda activate $ENV
        displayName: Setup environment
        env:
          ENV: $(CondaEnvironment)
      - bash: |
          pytest --disable-warnings -rp -vv --cov-report xml:cov.xml --cov=. -o junit_family=xunit2 --junitxml=TEST-iris_model.xml tests/
        displayName: Execute unit tests
        env:
            SPARK_LOCAL_IP: 127.0.0.1
            DATABRICKS_ADDRESS: $(DatabricksUrl)
            DATABRICKS_API_TOKEN: $(DatabricksToken)
            DATABRICKS_ORG: $(DatabricksOrg)
            DATABRICKS_CLUSTER_ID: $(DatabricksCluster)
            DATABRICKS_PORT: 15001
      - bash: |
          python setup.py bdist_wheel
        displayName: Build library
      - task: CopyFiles@2
        displayName: Copy Assets to Staging Folder
        inputs:
          Contents: |
            driver/**.py
            **/TEST-*.xml
            dist/**.whl
          TargetFolder: $(Build.ArtifactStagingDirectory)
      - bash: |
          databricks workspace mkdirs /Experiments
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/dist dbfs:/models/
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/driver dbfs:/driver/
        displayName: Upload to Databricks
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - powershell:
          $run = @{
            "run_name"="datapipeline";
            "new_cluster"=@{
              "spark_version"="7.1.x-cpu-ml-scala2.12";
              "node_type_id"="Standard_F8s";
              "num_workers"=1;
            };
            "spark_python_task"="{
              "python_file"="dbfs:/driver/datapipeline.py";
            };"
          }

          $body = ($run | ConvertTo-Json).Replace("`"", "`"`""")
          $runId = databricks runs submit --json "$body" | ConvertFrom-Json
          if ($runId.run_id -eq $null) {
            Write-Host "##vso[task.logissue type=error]Unable to create run $runId"
            exit 1
          } else {
            Write-Host "Run ID $runId"
          }

          while ($true) {
            $state = databricks runs get --run-id $runId.run_id | ConvertFrom-Json
            $s = $state.state.life_cycle_state
            Write-Host $s
            if ($s -eq "TERMINATED") {
              break
            } elseif ($s -eq "SKIPPED") {
              Write-Host "##vso[task.logissue type=error]Databricks processing SKIPPED"
              exit 1
            } elseif ($s -eq "INTERNAL_ERROR") {
              Write-Host "##vso[task.logissue type=error]Databricks processing INTERNAL_ERROR $($state.state.state_message)"
              exit 1
            }
            Sleep -Seconds 5
          }
        displayName: Execute data pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)

      - powershell:
          $run = @{
            "run_name"="features";
            "new_cluster"=@{
              "spark_version"="7.1.x-cpu-ml-scala2.12";
              "node_type_id"="Standard_F8s";
              "num_workers"=1;
            };
            "spark_python_task"="{
              "python_file"="dbfs:/driver/features.py";
            };"
          }

          $body = ($run | ConvertTo-Json).Replace("`"", "`"`""")
          $runId = databricks runs submit --json "$body" | ConvertFrom-Json
          if ($runId.run_id -eq $null) {
            Write-Host "##vso[task.logissue type=error]Unable to create run $runId"
            exit 1
          } else {
            Write-Host "Run ID $runId"
          }

          while ($true) {
            $state = databricks runs get --run-id $runId.run_id | ConvertFrom-Json
            $s = $state.state.life_cycle_state
            Write-Host $s
            if ($s -eq "TERMINATED") {
              break
            } elseif ($s -eq "SKIPPED") {
              Write-Host "##vso[task.logissue type=error]Databricks processing SKIPPED"
              exit 1
            } elseif ($s -eq "INTERNAL_ERROR") {
              Write-Host "##vso[task.logissue type=error]Databricks processing INTERNAL_ERROR $($state.state.state_message)"
              exit 1
            }
            Sleep -Seconds 5
          }
        displayName: Execute feature pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      
      - powershell:
          $run = @{
            "run_name"="training";
            "new_cluster"=@{
              "spark_version"="7.1.x-cpu-ml-scala2.12";
              "node_type_id"="Standard_F8s";
              "num_workers"=1;
            };
            "spark_python_task"="{
              "python_file"="dbfs:/driver/training.py";
            };"
          }

          $body = ($run | ConvertTo-Json).Replace("`"", "`"`""")
          $runId = databricks runs submit --json "$body" | ConvertFrom-Json
          if ($runId.run_id -eq $null) {
            Write-Host "##vso[task.logissue type=error]Unable to create run $runId"
            exit 1
          } else {
            Write-Host "Run ID $runId"
          }

          while ($true) {
            $state = databricks runs get --run-id $runId.run_id | ConvertFrom-Json
            $s = $state.state.life_cycle_state
            Write-Host $s
            if ($s -eq "TERMINATED") {
              break
            } elseif ($s -eq "SKIPPED") {
              Write-Host "##vso[task.logissue type=error]Databricks processing SKIPPED"
              exit 1
            } elseif ($s -eq "INTERNAL_ERROR") {
              Write-Host "##vso[task.logissue type=error]Databricks processing INTERNAL_ERROR $($state.state.state_message)"
              exit 1
            }
            Sleep -Seconds 5
          }
        displayName: Execute training pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)

      - powershell:
          $run = @{
            "run_name"="serving";
            "new_cluster"=@{
              "spark_version"="7.1.x-cpu-ml-scala2.12";
              "node_type_id"="Standard_F8s";
              "num_workers"=1;
            };
            "spark_python_task"="{
              "python_file"="dbfs:/driver/serving.py";
            };"
          }

          $body = ($run | ConvertTo-Json).Replace("`"", "`"`""")
          $runId = databricks runs submit --json "$body" | ConvertFrom-Json
          if ($runId.run_id -eq $null) {
            Write-Host "##vso[task.logissue type=error]Unable to create run $runId"
            exit 1
          } else {
            Write-Host "Run ID $runId"
          }

          while ($true) {
            $state = databricks runs get --run-id $runId.run_id | ConvertFrom-Json
            $s = $state.state.life_cycle_state
            Write-Host $s
            if ($s -eq "TERMINATED") {
              break
            } elseif ($s -eq "SKIPPED") {
              Write-Host "##vso[task.logissue type=error]Databricks processing SKIPPED"
              exit 1
            } elseif ($s -eq "INTERNAL_ERROR") {
              Write-Host "##vso[task.logissue type=error]Databricks processing INTERNAL_ERROR $($state.state.state_message)"
              exit 1
            }
            Sleep -Seconds 5
          }
        displayName: Execute serving pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)

      - task: PublishBuildArtifacts@1
        displayName: Publish Artefacts
        inputs:
          PathtoPublish: $(Build.ArtifactStagingDirectory)
          ArtifactName: drop
          publishLocation: Container
      - task: PublishTestResults@2
        inputs:
            testResultsFormat: JUnit
            testResultsFiles: '**/TEST-*.xml'
            searchFolder: $(Build.ArtifactStagingDirectory)/drop/
            mergeTestResults: true
            failTaskOnFailedTests: true
            testRunTitle: Pipeline Tests


