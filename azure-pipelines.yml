trigger: 
- master

stages:

- stage: Build
  displayName: Build artefacts
  variables:
    - group: dev
  pool: 
    vm-image: ubuntu-latest
  jobs:
  - job:
    displayName: Create artefact
    steps:
      - bash: |
          echo {} > ~/.databricks-connect
          curl -L https://pyenv.run | bash
          export PATH="/home/vsts/.pyenv/bin:$PATH"
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"
          pyenv install 3.7.6
          pyenv global 3.7.6
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"
          python -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt
          python setup.py develop   
        displayName: Setup environment     
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          pylint src/iris_model
        displayName: Execute linting rules (pylint)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          flake8 src/iris_model
        displayName: Execute linting rules (flake8)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          pytest --disable-warnings -rp -vv --cov-report term --cov-report xml:cov.xml --cov=. -o junit_family=xunit2 --junitxml=TEST-iris_model.xml tests/
        displayName: Execute unit tests
        env:
            SPARK_LOCAL_IP: 127.0.0.1
            DATABRICKS_ADDRESS: $(DatabricksUrl)
            DATABRICKS_API_TOKEN: $(DatabricksToken)
            DATABRICKS_ORG: $(DatabricksOrg)
            DATABRICKS_CLUSTER_ID: $(DatabricksCluster)
            DATABRICKS_PORT: 15001
      - task: PublishTestResults@2
        inputs:
            testResultsFormat: JUnit
            testResultsFiles: '**/TEST-*.xml'
            mergeTestResults: true
            failTaskOnFailedTests: true
            testRunTitle: Pipeline Tests
      - task: PublishCodeCoverageResults@1
        inputs:
          codeCoverageTool: 'Cobertura'
          summaryFileLocation: '**/cov.xml'
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          python setup.py bdist_wheel
        displayName: Build library
      - task: CopyFiles@2
        displayName: Copy Assets to Staging Folder
        inputs:
          Contents: |
            scripts/**py
            driver/**.py
            **/TEST-*.xml
            dist/**.whl
          TargetFolder: $(Build.ArtifactStagingDirectory)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          databricks workspace mkdirs /Experiments
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/dist dbfs:/models/
          databricks fs cp --recursive --overwrite $(Build.ArtifactStagingDirectory)/driver dbfs:/driver/
        displayName: Upload to Databricks
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)      
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          python scripts/submit.py --file dbfs:/driver/datapipeline.py
        displayName: Execute data pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          python scripts/submit.py --file dbfs:/driver/features.py
        displayName: Execute features pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          python scripts/submit.py --file dbfs:/driver/training.py
        displayName: Execute training pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - bash: |
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"          
          source .venv/bin/activate
          python scripts/submit.py --file dbfs:/driver/serving.py
        displayName: Execute serving pipeline
        env:
            DATABRICKS_HOST: $(DatabricksUrl)
            DATABRICKS_TOKEN: $(DatabricksToken)
      - task: PublishBuildArtifacts@1
        displayName: Publish Artefacts
        inputs:
          PathtoPublish: $(Build.ArtifactStagingDirectory)
          ArtifactName: drop
          publishLocation: Container


